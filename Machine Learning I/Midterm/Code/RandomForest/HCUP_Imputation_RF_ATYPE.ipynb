{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:\n",
    "Data cleaning and preparation; admittedly, this is not the most efficient code -- but it will produce what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in dataframe\n",
    "df = pd.read_csv(\"data_train.csv\")\n",
    "\n",
    "#Steps to clean data by dropping columns where the number of empty rows is >= 445,000\n",
    "dfComplete = df.dropna(1, thresh= 445000)\n",
    "# dfComplete.shape #Results in (494932,58)\n",
    "\n",
    "#Steps to clean data by dropping row where the number of empty columns is > 0\n",
    "dfCompleteAll = dfComplete.dropna(0, how=\"any\")\n",
    "# dfCompleteAll.shape #results in a dataframe of (476155,58)\n",
    "\n",
    "# dfCompleteAll.isnull().sum() #no more nulls in the dataset\n",
    "\n",
    "y = dfCompleteAll[[\"ASOURCE\", \"ATYPE\", \"RACE\", \"TOTCHG\", \"ZIPINC_QRTL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"The next step is to prepare variables for exploratory data analysis and feature\n",
    "selection using a random forest. \n",
    "\n",
    "Random forest does not require standardization of continuous variables or normalization\n",
    "of discrete variable. For categorical features, we will need to use pd.get_dummies or \n",
    "one hot encoding to create binary dummy variables. \n",
    "\"\"\"\n",
    "\n",
    "#First create two dataframes of int and float values to make things easier to work with \n",
    "columnNames = dfCompleteAll.columns\n",
    "dfFloat = pd.DataFrame()\n",
    "dfInt = pd.DataFrame()\n",
    "for name in columnNames:\n",
    "    if dfCompleteAll[name].dtype == float:\n",
    "        dfFloat = dfFloat.join(dfCompleteAll[name], how = \"right\")\n",
    "    else:\n",
    "        dfInt = dfInt.join(dfCompleteAll[name], how = \"right\")\n",
    "        \n",
    "#Convert all columns in DfFloat, except DISCWT, to integer values. Afterwards nominal features\n",
    "#will be one-hot encoded to create dummy variables, again we will not normalize or stadardize\n",
    "# numeric values. \n",
    "\n",
    "float_toInt = ['AGE', 'AMONTH', 'AWEEKEND', 'DIED', 'DISPUNIFORM', 'DXCCS1',\n",
    "       'DXCCS2', 'FEMALE', 'LOS', 'PAY1', 'HOSP_BEDSIZE', 'HOSP_CONTROL',\n",
    "       'HOSP_LOCTEACH']\n",
    "\n",
    "for digit in float_toInt:\n",
    "    dfFloat[digit] = dfFloat[digit].astype(int)\n",
    "    \n",
    "# In dfFloat we have the following columns and feature groupings. \n",
    "# For reference use feature_desc or call .unique() method on one of the columns\n",
    "\n",
    "# CONTINUOUS:\n",
    "dfFloatContinuous = dfFloat[[\"AGE\", \"DISCWT\"]]\n",
    "\n",
    "\n",
    "# NOMINAL:\n",
    "dfFloatNominal = dfFloat[['AMONTH', 'DISPUNIFORM', 'DXCCS1',\n",
    "       'DXCCS2', 'PAY1', 'HOSP_CONTROL','HOSP_LOCTEACH']]\n",
    "\n",
    "# BINARY & ORDINAL\n",
    "dfBinaryOrdinal = dfFloat[[\"DIED\", \"AWEEKEND\", \"FEMALE\", \"HOSP_BEDSIZE\"]]\n",
    "\n",
    "# DISCRETE\n",
    "dfDiscrete = dfFloat[[\"LOS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(475155, 556)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use pd.get_dummies to turn nominal variables into dummy variables by first setting all \n",
    "#values as string, a requirement of pd.get_dummies. \n",
    "\n",
    "dfFloatNominal = dfFloatNominal.loc[:].astype(str)\n",
    "    \n",
    "dfFloatNominal = pd.get_dummies(dfFloatNominal)\n",
    "dfFloatNominal.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(475155, 563)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now recreate the original dfFloat dataframe as dfFloatPreprocessed which \n",
    "# will have variables ready for feature selection with RF. Next the same thing must be down\n",
    "#with dfInt\n",
    "list_of_dataframes = [dfFloatContinuous, dfBinaryOrdinal, dfDiscrete, dfFloatNominal]\n",
    "\n",
    "dfFloatPreprocessed = pd.DataFrame()\n",
    "for frame in list_of_dataframes:\n",
    "    dfFloatPreprocessed = dfFloatPreprocessed.join(frame, how = \"right\")\n",
    "dfFloatPreprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare dfInt for preprocessing, starting with dropping respone variables\n",
    "dfInt = dfInt.drop([\"ASOURCE\", \"ATYPE\", \"RACE\", \"TOTCHG\", \"ZIPINC_QRTL\"], axis= 1)\n",
    "\n",
    "#CMs are all binary, therefore, create a separate dataframe for these columns: \n",
    "dfCm = dfInt[['CM_AIDS', 'CM_ALCOHOL', 'CM_ANEMDEF',\n",
    "       'CM_ARTH', 'CM_BLDLOSS', 'CM_CHF', 'CM_CHRNLUNG', 'CM_COAG',\n",
    "       'CM_DEPRESS', 'CM_DM', 'CM_DMCX', 'CM_DRUG', 'CM_HTN_C', 'CM_HYPOTHY',\n",
    "       'CM_LIVER', 'CM_LYMPH', 'CM_LYTES', 'CM_METS', 'CM_NEURO', 'CM_OBESE',\n",
    "       'CM_PARA', 'CM_PERIVASC', 'CM_PSYCH', 'CM_PULMCIRC', 'CM_RENLFAIL',\n",
    "       'CM_TUMOR', 'CM_ULCER', 'CM_VALVE', 'CM_WGHTLOSS']]\n",
    "\n",
    "#Update the dfInt dataframe to a new dataframe:\n",
    "dfIntShort = dfInt.drop(['CM_AIDS', 'CM_ALCOHOL', 'CM_ANEMDEF',\n",
    "       'CM_ARTH', 'CM_BLDLOSS', 'CM_CHF', 'CM_CHRNLUNG', 'CM_COAG',\n",
    "       'CM_DEPRESS', 'CM_DM', 'CM_DMCX', 'CM_DRUG', 'CM_HTN_C', 'CM_HYPOTHY',\n",
    "       'CM_LIVER', 'CM_LYMPH', 'CM_LYTES', 'CM_METS', 'CM_NEURO', 'CM_OBESE',\n",
    "       'CM_PARA', 'CM_PERIVASC', 'CM_PSYCH', 'CM_PULMCIRC', 'CM_RENLFAIL',\n",
    "       'CM_TUMOR', 'CM_ULCER', 'CM_VALVE', 'CM_WGHTLOSS'],axis = 1)\n",
    "\n",
    "#Update the dfInt dataframe to a new dataframe:\n",
    "dfIntShort = dfInt.drop(['CM_AIDS', 'CM_ALCOHOL', 'CM_ANEMDEF',\n",
    "       'CM_ARTH', 'CM_BLDLOSS', 'CM_CHF', 'CM_CHRNLUNG', 'CM_COAG',\n",
    "       'CM_DEPRESS', 'CM_DM', 'CM_DMCX', 'CM_DRUG', 'CM_HTN_C', 'CM_HYPOTHY',\n",
    "       'CM_LIVER', 'CM_LYMPH', 'CM_LYTES', 'CM_METS', 'CM_NEURO', 'CM_OBESE',\n",
    "       'CM_PARA', 'CM_PERIVASC', 'CM_PSYCH', 'CM_PULMCIRC', 'CM_RENLFAIL',\n",
    "       'CM_TUMOR', 'CM_ULCER', 'CM_VALVE', 'CM_WGHTLOSS'],axis = 1)\n",
    "\n",
    "#Since NDX, NPR, ORPROC, TOTAL_DISC are all either Binary or Discerete variables, create a separate dataframe\n",
    "dfIntBinaryDiscrete = dfIntShort[[\"NDX\", \"NPR\", \"ORPROC\", \"TOTAL_DISC\"]]\n",
    "\n",
    "# Since DQTR, HOSPID, MDC, NIS_STRATUM, HOSP_REGION are all nominal variables, create a separate dataframe to \n",
    "#turn these into dummy variables. ALSO Drop \"KEY\" as this is the record id field: \n",
    "dfIntToDummies = dfIntShort.drop([\"KEY\", \"NDX\", \"NPR\", \"ORPROC\", \"TOTAL_DISC\"], axis= 1)\n",
    "\n",
    "#Turn values in DQTR, HOSPID, MDC, NIS_STRATUM, HOSP_REGION to string\n",
    "dfIntToDummies = dfIntToDummies.loc[:].astype(str)\n",
    "\n",
    "#Use pd.get_dummies to turn nominal string values to binary dummy variables\n",
    "dfIntToDummies = pd.get_dummies(dfIntToDummies)\n",
    "# dfIntToDummies.head()\n",
    "\n",
    "intRecombine = [dfIntToDummies, dfCm, dfIntBinaryDiscrete]\n",
    "\n",
    "dfIntPreprocessed = pd.DataFrame()\n",
    "for df in intRecombine:\n",
    "    dfIntPreprocessed = dfIntPreprocessed.join(df, how = \"right\")\n",
    "    \n",
    "dfPreprocessed = dfFloatPreprocessed.join(dfIntPreprocessed, how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plug these dfs back together\n",
    "dfFull = pd.concat([dfPreprocessed, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Outcome Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, separate our variables\n",
    "X = dfFull.drop(['ATYPE'], axis=1)\n",
    "y = dfFull.ATYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    255366\n",
       "3    106770\n",
       "2     87084\n",
       "4     24558\n",
       "6       705\n",
       "5       672\n",
       "Name: ATYPE, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.537437\n",
       "3    0.224706\n",
       "2    0.183275\n",
       "4    0.051684\n",
       "6    0.001484\n",
       "5    0.001414\n",
       "Name: ATYPE, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These class distributions are **highly imbalanced. We may want to consider oversampling our data with synthetic data creation packages.** But first, let's make a train/test split so regardless of what we do with oversampling, there's no information bleed. \n",
    "\n",
    "Then let's fit a \"base\" model random forest on the full data so we can compare subsequent models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Base\" Model Random Forest\n",
    "- In reality, this base model isn't at all a blind stab; we have methodically tuned Random Forests to predict other outcome variables in this dataset, and **we're starting with ideal parameters for models that imputed RACE, ASOURCE, etc.** (See the accompanying .ipynb \"HCUP_Imputation_RF_Undersample_RACE_ZIPINCQRTL\" for details).\n",
    "- We want to see if we can beat this base model by either/both:\n",
    "    - Oversampling small classes\n",
    "    - Additional tuning to find new ideal parameters for this variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=250, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=True, random_state=12, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rfc = RandomForestClassifier(n_jobs=-1,max_features=100,n_estimators=300,max_depth=250,min_samples_split=2,\n",
    "                             min_samples_leaf=1,oob_score = True,random_state=12)\n",
    "\n",
    "base_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89542359861518872"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rfc_predictions = base_rfc.predict(X_test)\n",
    "base_rfc_accuracy = accuracy_score(y_test, base_rfc_predictions)\n",
    "base_rfc_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.91      0.95      0.93     51061\n",
      "          2       0.84      0.76      0.80     17497\n",
      "          3       0.89      0.85      0.87     21231\n",
      "          4       0.99      1.00      1.00      4977\n",
      "          5       0.84      0.47      0.60       132\n",
      "          6       0.84      0.56      0.67       133\n",
      "\n",
      "avg / total       0.89      0.90      0.89     95031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_rfc_report = classification_report(y_test, base_rfc_predictions)\n",
    "print(base_rfc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted ATYPE</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual ATYPE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48688</td>\n",
       "      <td>1259</td>\n",
       "      <td>1098</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3019</td>\n",
       "      <td>13256</td>\n",
       "      <td>1208</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1865</td>\n",
       "      <td>1298</td>\n",
       "      <td>18047</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted ATYPE      1      2      3     4   5   6\n",
       "Actual ATYPE                                      \n",
       "1                48688   1259   1098     2  11   3\n",
       "2                 3019  13256   1208    12   1   1\n",
       "3                 1865   1298  18047    11   0  10\n",
       "4                    3      7      1  4966   0   0\n",
       "5                   69      0      1     0  62   0\n",
       "6                   24     28      7     0   0  74"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, base_rfc_predictions, \n",
    "            rownames=['Actual ATYPE'], colnames=['Predicted ATYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model does shockingly well.** So well that I had to double check that I didn't accidentally train it on test data.\n",
    "\n",
    "Still, let's see if we can tune it at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Sample a Subset of Our Data\n",
    "- I have an early 2010s MacBook Air, so I can't run a grid search on 300k observation datasets.\n",
    "- We'll do a grid search on this smaller subset of the data to find ideal params. Then we'll try them on our full data.\n",
    "- The **RISK** is that in a smaller sample, there are so few class 4 and 5 observations that we're tuning a model based on what is effectively a different dataset. That's why we instantiated a base model (which really did quite well) to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = dfFull.sample(n=30000)\n",
    "X_sample = sample.drop(['ATYPE'], axis=1)\n",
    "y_sample = sample.ATYPE\n",
    "\n",
    "X_sample_train, X_sample_test, y_sample_train, y_sample_test = train_test_split(X_sample, y_sample, \n",
    "                                                                                test_size = 0.25, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12061\n",
       "3     4987\n",
       "2     4154\n",
       "4     1231\n",
       "6       41\n",
       "5       26\n",
       "Name: ATYPE, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yeah, worried that 6 and 5 are so sparse that our tuning will be biased. We'll see!\n",
    "y_sample_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=250, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=True, random_state=12, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'min_samples_split': [2, 5, 10, 25], 'max_features': ['auto', 100, 300]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pick our params; from (bad) experience, my machine can handle about 7 different params\n",
    "param_grid = {\n",
    "    'min_samples_split': [2, 5, 10, 25],\n",
    "    'max_features': ['auto', 100, 300]\n",
    "}\n",
    "\n",
    "rfc_CV = GridSearchCV(estimator=base_rfc, param_grid=param_grid, scoring='f1_micro', \n",
    "                      n_jobs=-1, return_train_score=False)\n",
    "\n",
    "rfc_CV.fit(X_sample_train, y_sample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 300, 'min_samples_split': 5}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88228051898854054"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_CV_predictions = rfc_CV.predict(X_test)\n",
    "rfc_CV_accuracy = accuracy_score(y_test, rfc_CV_predictions)\n",
    "rfc_CV_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, this is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.90      0.94      0.92     51061\n",
      "          2       0.83      0.73      0.78     17497\n",
      "          3       0.86      0.84      0.85     21231\n",
      "          4       0.99      1.00      1.00      4977\n",
      "          5       1.00      0.07      0.13       132\n",
      "          6       0.81      0.56      0.66       133\n",
      "\n",
      "avg / total       0.88      0.88      0.88     95031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_CV_report = classification_report(y_test, rfc_CV_predictions)\n",
    "print(rfc_CV_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No prediction improvement over the base model, because this model hasn't seen enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Best Params\" Random Forest\n",
    "- Remember, it's entirely possible this will not improve results, as the CV was biased by computational necessity.\n",
    "- Then again, it could have provided valuable insight. We'll see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=250, max_features=300, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=5,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=True, random_state=12, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unfortunately, this times out at these settings unless we reduce the training set; guess 300 features is too much\n",
    "#this will still be generalizable - we are holding out a BIGGER test set after all\n",
    "#we just hope our training data at 55% of the dataset is enough\n",
    "\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X, y, test_size = 0.45, random_state = 12)\n",
    "\n",
    "rfc_tuned = RandomForestClassifier(n_jobs=-1,max_features=300,n_estimators=300,max_depth=250,min_samples_split=5,\n",
    "                             min_samples_leaf=1,oob_score = True,random_state=12)\n",
    "\n",
    "rfc_tuned.fit(X_train_new, y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 3, ..., 3, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_tuned_predictions = rfc_tuned.predict(X_test)\n",
    "rfc_tuned_accuracy = accuracy_score(y_test, rfc_tuned_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89452915364460017"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_tuned_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.91      0.95      0.93     51061\n",
      "          2       0.83      0.76      0.79     17497\n",
      "          3       0.88      0.85      0.87     21231\n",
      "          4       1.00      1.00      1.00      4977\n",
      "          5       0.79      0.52      0.62       132\n",
      "          6       0.84      0.55      0.66       133\n",
      "\n",
      "avg / total       0.89      0.89      0.89     95031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_tuned_report = classification_report(y_test, rfc_tuned_predictions)\n",
    "print(rfc_tuned_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison:\n",
    "\n",
    "Strictly according to F1 scores, the tuned model has not improved upon our \"base\" model. We see some slightly lower precision/recall scores (.01) that are very likely just random chance. However, **it has distinguishing characteristics that could make it useful, depending on domain needs.**.\n",
    "- The tuned model has *lower* precision but *higher* recall on class = 5. It is predicting 5 more often. It results in a lower f1 score, but depending on the nature of a problem, one sometimes wants to sacrifice precision for recall. We don't have the domain knowledge in health care research to know here, but it's good to have a choice in models.\n",
    "- We also suspect that if, computationally, we were allow this tuned model to \"see\" more data that it would perform even better.\n",
    "\n",
    "**\"Tuned\" Model:**\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.91      0.95      0.93     51061\n",
    "          2       0.83      0.76      0.79     17497\n",
    "          3       0.88      0.85      0.87     21231\n",
    "          4       1.00      1.00      1.00      4977\n",
    "          5       0.79      0.52      0.62       132\n",
    "          6       0.84      0.55      0.66       133\n",
    "\n",
    "    avg / total       0.89      0.89      0.89     95031\n",
    "\n",
    "**\"Base\" Model:**\n",
    "\n",
    "                precision    recall  f1-score   support\n",
    "\n",
    "          1       0.91      0.95      0.93     51061\n",
    "          2       0.84      0.76      0.80     17497\n",
    "          3       0.89      0.85      0.87     21231\n",
    "          4       0.99      1.00      1.00      4977\n",
    "          5       0.84      0.47      0.60       132\n",
    "          6       0.84      0.56      0.67       133\n",
    "\n",
    "    avg / total       0.89      0.90      0.89     95031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
