{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:\n",
    "Data cleaning and preparation; admittedly, this is not the most efficient code -- but it will produce what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read in dataframe\n",
    "df = pd.read_csv(\"data_train.csv\")\n",
    "\n",
    "#Steps to clean data by dropping columns where the number of empty rows is >= 445,000\n",
    "dfComplete = df.dropna(1, thresh= 445000)\n",
    "# dfComplete.shape #Results in (494932,58)\n",
    "\n",
    "#Steps to clean data by dropping row where the number of empty columns is > 0\n",
    "dfCompleteAll = dfComplete.dropna(0, how=\"any\")\n",
    "# dfCompleteAll.shape #results in a dataframe of (476155,58)\n",
    "\n",
    "# dfCompleteAll.isnull().sum() #no more nulls in the dataset\n",
    "\n",
    "y = dfCompleteAll[[\"ASOURCE\", \"ATYPE\", \"RACE\", \"TOTCHG\", \"ZIPINC_QRTL\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"The next step is to prepare variables for exploratory data analysis and feature\n",
    "selection using a random forest. \n",
    "\n",
    "Random forest does not require standardization of continuous variables or normalization\n",
    "of discrete variable. For categorical features, we will need to use pd.get_dummies or \n",
    "one hot encoding to create binary dummy variables. \n",
    "\"\"\"\n",
    "\n",
    "#First create two dataframes of int and float values to make things easier to work with \n",
    "columnNames = dfCompleteAll.columns\n",
    "dfFloat = pd.DataFrame()\n",
    "dfInt = pd.DataFrame()\n",
    "for name in columnNames:\n",
    "    if dfCompleteAll[name].dtype == float:\n",
    "        dfFloat = dfFloat.join(dfCompleteAll[name], how = \"right\")\n",
    "    else:\n",
    "        dfInt = dfInt.join(dfCompleteAll[name], how = \"right\")\n",
    "        \n",
    "#Convert all columns in DfFloat, except DISCWT, to integer values. Afterwards nominal features\n",
    "#will be one-hot encoded to create dummy variables, again we will not normalize or stadardize\n",
    "# numeric values. \n",
    "\n",
    "float_toInt = ['AGE', 'AMONTH', 'AWEEKEND', 'DIED', 'DISPUNIFORM', 'DXCCS1',\n",
    "       'DXCCS2', 'FEMALE', 'LOS', 'PAY1', 'HOSP_BEDSIZE', 'HOSP_CONTROL',\n",
    "       'HOSP_LOCTEACH']\n",
    "\n",
    "for digit in float_toInt:\n",
    "    dfFloat[digit] = dfFloat[digit].astype(int)\n",
    "    \n",
    "# In dfFloat we have the following columns and feature groupings. \n",
    "# For reference use feature_desc or call .unique() method on one of the columns\n",
    "\n",
    "# CONTINUOUS:\n",
    "dfFloatContinuous = dfFloat[[\"AGE\", \"DISCWT\"]]\n",
    "\n",
    "\n",
    "# NOMINAL:\n",
    "dfFloatNominal = dfFloat[['AMONTH', 'DISPUNIFORM', 'DXCCS1',\n",
    "       'DXCCS2', 'PAY1', 'HOSP_CONTROL','HOSP_LOCTEACH']]\n",
    "\n",
    "# BINARY & ORDINAL\n",
    "dfBinaryOrdinal = dfFloat[[\"DIED\", \"AWEEKEND\", \"FEMALE\", \"HOSP_BEDSIZE\"]]\n",
    "\n",
    "# DISCRETE\n",
    "dfDiscrete = dfFloat[[\"LOS\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(475155, 556)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use pd.get_dummies to turn nominal variables into dummy variables by first setting all \n",
    "#values as string, a requirement of pd.get_dummies. \n",
    "\n",
    "dfFloatNominal = dfFloatNominal.loc[:].astype(str)\n",
    "    \n",
    "dfFloatNominal = pd.get_dummies(dfFloatNominal)\n",
    "dfFloatNominal.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(475155, 563)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now recreate the original dfFloat dataframe as dfFloatPreprocessed which \n",
    "# will have variables ready for feature selection with RF. Next the same thing must be down\n",
    "#with dfInt\n",
    "list_of_dataframes = [dfFloatContinuous, dfBinaryOrdinal, dfDiscrete, dfFloatNominal]\n",
    "\n",
    "dfFloatPreprocessed = pd.DataFrame()\n",
    "for frame in list_of_dataframes:\n",
    "    dfFloatPreprocessed = dfFloatPreprocessed.join(frame, how = \"right\")\n",
    "dfFloatPreprocessed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prepare dfInt for preprocessing, starting with dropping respone variables\n",
    "dfInt = dfInt.drop([\"ASOURCE\", \"ATYPE\", \"RACE\", \"TOTCHG\", \"ZIPINC_QRTL\"], axis= 1)\n",
    "\n",
    "#CMs are all binary, therefore, create a separate dataframe for these columns: \n",
    "dfCm = dfInt[['CM_AIDS', 'CM_ALCOHOL', 'CM_ANEMDEF',\n",
    "       'CM_ARTH', 'CM_BLDLOSS', 'CM_CHF', 'CM_CHRNLUNG', 'CM_COAG',\n",
    "       'CM_DEPRESS', 'CM_DM', 'CM_DMCX', 'CM_DRUG', 'CM_HTN_C', 'CM_HYPOTHY',\n",
    "       'CM_LIVER', 'CM_LYMPH', 'CM_LYTES', 'CM_METS', 'CM_NEURO', 'CM_OBESE',\n",
    "       'CM_PARA', 'CM_PERIVASC', 'CM_PSYCH', 'CM_PULMCIRC', 'CM_RENLFAIL',\n",
    "       'CM_TUMOR', 'CM_ULCER', 'CM_VALVE', 'CM_WGHTLOSS']]\n",
    "\n",
    "#Update the dfInt dataframe to a new dataframe:\n",
    "dfIntShort = dfInt.drop(['CM_AIDS', 'CM_ALCOHOL', 'CM_ANEMDEF',\n",
    "       'CM_ARTH', 'CM_BLDLOSS', 'CM_CHF', 'CM_CHRNLUNG', 'CM_COAG',\n",
    "       'CM_DEPRESS', 'CM_DM', 'CM_DMCX', 'CM_DRUG', 'CM_HTN_C', 'CM_HYPOTHY',\n",
    "       'CM_LIVER', 'CM_LYMPH', 'CM_LYTES', 'CM_METS', 'CM_NEURO', 'CM_OBESE',\n",
    "       'CM_PARA', 'CM_PERIVASC', 'CM_PSYCH', 'CM_PULMCIRC', 'CM_RENLFAIL',\n",
    "       'CM_TUMOR', 'CM_ULCER', 'CM_VALVE', 'CM_WGHTLOSS'],axis = 1)\n",
    "\n",
    "#Update the dfInt dataframe to a new dataframe:\n",
    "dfIntShort = dfInt.drop(['CM_AIDS', 'CM_ALCOHOL', 'CM_ANEMDEF',\n",
    "       'CM_ARTH', 'CM_BLDLOSS', 'CM_CHF', 'CM_CHRNLUNG', 'CM_COAG',\n",
    "       'CM_DEPRESS', 'CM_DM', 'CM_DMCX', 'CM_DRUG', 'CM_HTN_C', 'CM_HYPOTHY',\n",
    "       'CM_LIVER', 'CM_LYMPH', 'CM_LYTES', 'CM_METS', 'CM_NEURO', 'CM_OBESE',\n",
    "       'CM_PARA', 'CM_PERIVASC', 'CM_PSYCH', 'CM_PULMCIRC', 'CM_RENLFAIL',\n",
    "       'CM_TUMOR', 'CM_ULCER', 'CM_VALVE', 'CM_WGHTLOSS'],axis = 1)\n",
    "\n",
    "#Since NDX, NPR, ORPROC, TOTAL_DISC are all either Binary or Discerete variables, create a separate dataframe\n",
    "dfIntBinaryDiscrete = dfIntShort[[\"NDX\", \"NPR\", \"ORPROC\", \"TOTAL_DISC\"]]\n",
    "\n",
    "# Since DQTR, HOSPID, MDC, NIS_STRATUM, HOSP_REGION are all nominal variables, create a separate dataframe to \n",
    "#turn these into dummy variables. ALSO Drop \"KEY\" as this is the record id field: \n",
    "dfIntToDummies = dfIntShort.drop([\"KEY\", \"NDX\", \"NPR\", \"ORPROC\", \"TOTAL_DISC\"], axis= 1)\n",
    "\n",
    "#Turn values in DQTR, HOSPID, MDC, NIS_STRATUM, HOSP_REGION to string\n",
    "dfIntToDummies = dfIntToDummies.loc[:].astype(str)\n",
    "\n",
    "#Use pd.get_dummies to turn nominal string values to binary dummy variables\n",
    "dfIntToDummies = pd.get_dummies(dfIntToDummies)\n",
    "# dfIntToDummies.head()\n",
    "\n",
    "intRecombine = [dfIntToDummies, dfCm, dfIntBinaryDiscrete]\n",
    "\n",
    "dfIntPreprocessed = pd.DataFrame()\n",
    "for df in intRecombine:\n",
    "    dfIntPreprocessed = dfIntPreprocessed.join(df, how = \"right\")\n",
    "    \n",
    "dfPreprocessed = dfFloatPreprocessed.join(dfIntPreprocessed, how = \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plug these dfs back together\n",
    "dfFull = pd.concat([dfPreprocessed, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check ASOURCE Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#first, separate our variables\n",
    "X = dfFull.drop(['ASOURCE'], axis=1)\n",
    "y = dfFull.ASOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.678164\n",
       "1    0.255487\n",
       "2    0.035982\n",
       "3    0.030121\n",
       "4    0.000246\n",
       "Name: ASOURCE, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take a look at distribution in this outcome variable\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    322233\n",
       "1    121396\n",
       "2     17097\n",
       "3     14312\n",
       "4       117\n",
       "Name: ASOURCE, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yikes; get raw counts\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change these sampling distributions. Ideally, we'd take a random undersample of our majority classes. But we don't even have enough class = 4 for that; we're going to have to make up some data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "- Must do this **before** oversampling, or else we'll experience information bleed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now X_test and y_test will be completely held out. They are our real, observed data that the model hasn't seen. Time to mess with our sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    257836\n",
       "1     97200\n",
       "2     13641\n",
       "3     11353\n",
       "4        94\n",
       "Name: ASOURCE, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just oversample 4 by creating synthetic samples with the ADASYN method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada = ADASYN(n_jobs = -1,  ratio = {1:97200, 2:13641, 3:11353, 4:10000, 5:257836})\n",
    "#The values of the dict passed to Ratio correspond to the desired number of samples.\n",
    "\n",
    "X_train_rs, y_train_rs = ada.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    257836\n",
      "1     97200\n",
      "2     13641\n",
      "3     11353\n",
      "4     10016\n",
      "dtype: int64 \n",
      "\n",
      "5    0.661040\n",
      "1    0.249201\n",
      "2    0.034973\n",
      "3    0.029107\n",
      "4    0.025679\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "new_counts = pd.Series(y_train_rs) #ADASYN package produces an ndarray\n",
    "print(new_counts.value_counts(), \"\\n\")\n",
    "print(new_counts.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have to be good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a Random Forest \"base model\"\n",
    "- Before cross validation, we want to establish a base metric for performance.\n",
    "- To start, we'll use parameters based on a tuned RF we fit to predict the RACE variable.\n",
    "    - But, we'll set min_samples_split to 2 to start so our trees in this forest can \"find\" these smaller classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=250, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=True, random_state=12, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_jobs=-1,max_features=100,n_estimators=300,max_depth=250,min_samples_split=2,\n",
    "                             min_samples_leaf=1,oob_score = True,random_state=12)\n",
    "\n",
    "clf.fit(X_train_rs, y_train_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94017741579063674"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_predictions = clf.predict(X_test)\n",
    "clf_accuracy = accuracy_score(y_test, clf_predictions)\n",
    "clf_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.97      0.94     24196\n",
      "          2       0.80      0.64      0.71      3456\n",
      "          3       0.72      0.35      0.47      2959\n",
      "          4       0.00      0.00      0.00        23\n",
      "          5       0.96      0.97      0.97     64397\n",
      "\n",
      "avg / total       0.94      0.94      0.94     95031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_report = classification_report(y_test, clf_predictions)\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted ASOURCE</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual ASOURCE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23474</td>\n",
       "      <td>56</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>315</td>\n",
       "      <td>2207</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>616</td>\n",
       "      <td>97</td>\n",
       "      <td>1044</td>\n",
       "      <td>0</td>\n",
       "      <td>1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1119</td>\n",
       "      <td>400</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>62621</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted ASOURCE      1     2     3  4      5\n",
       "Actual ASOURCE                                \n",
       "1                  23474    56    93  0    573\n",
       "2                    315  2207    58  0    876\n",
       "3                    616    97  1044  0   1202\n",
       "4                     16     0     1  0      6\n",
       "5                   1119   400   256  1  62621"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test, clf_predictions, rownames=['Actual ASOURCE'], colnames=['Predicted ASOURCE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty sharp model to start, *except* for class 4. There are 23 observed class 4s in our entire test set of 95,031 observations. So...not sure how realistic it is for our model to find them. In the future, we may want to use anomaly detection models for class = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning\n",
    "- We want to see if we can improve these F1 scores.\n",
    "- We'll have to get a smaller set of data so our computer can handle it. This will be a little tricky.\n",
    "    - First, we'll take a random sample of our real, observed data.\n",
    "    - Then, we'll impute some data in this new, smaller slice for our smallest minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfSample = dfFull.sample(n=25000, random_state=12)\n",
    "small_X = dfSample.drop(['ASOURCE'], axis=1)\n",
    "small_y = dfSample.ASOURCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    16958\n",
       "1     6355\n",
       "2      904\n",
       "3      775\n",
       "4        8\n",
       "Name: ASOURCE, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#well, we are just going to have to create synthetic data for classes 2,3 and 4 and hope it doesn't affect our \n",
    "#tuning's applicability to a model that uses synthetic data only for class 4\n",
    "\n",
    "new_oversample = ADASYN(n_jobs = -1,  ratio = {1:6355, 2:3000, 3:3000, 4:2000, 5:16958})\n",
    "\n",
    "X_train_red, y_train_red = new_oversample.fit_sample(small_X, small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=250, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=True, random_state=12, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'min_samples_split': [2, 5, 10, 25], 'max_features': ['log2', 'auto', 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "       scoring='f1_micro', verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'min_samples_split': [2, 5, 10, 25],\n",
    "    'max_features': ['log2', 'auto', 100]\n",
    "}\n",
    "\n",
    "    \n",
    "CV_clf = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', n_jobs=-1, \n",
    "                      return_train_score=False)\n",
    "\n",
    "CV_clf.fit(X_train_red, y_train_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 100, 'min_samples_split': 2}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, our ideal parameters are the ones we started with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideal Params Model on Raw Data for Comparison\n",
    "- Just to make sure we're getting value from our data resample, let's run an model with these ideal params on the (very) unevenly distributed actual training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=250, max_features=100, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=True, random_state=12, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_comp = RandomForestClassifier(n_jobs=-1,max_features=100,n_estimators=300,max_depth=250,min_samples_split=2,\n",
    "                             min_samples_leaf=1,oob_score = True,random_state=12)\n",
    "\n",
    "clf_comp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94009323273458134"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_comp_predictions = clf_comp.predict(X_test)\n",
    "clf_comp_accuracy = accuracy_score(y_test, clf_comp_predictions)\n",
    "clf_comp_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.92      0.97      0.94     24196\n",
      "          2       0.80      0.63      0.71      3456\n",
      "          3       0.72      0.35      0.47      2959\n",
      "          4       0.00      0.00      0.00        23\n",
      "          5       0.96      0.97      0.97     64397\n",
      "\n",
      "avg / total       0.94      0.94      0.94     95031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_comp_report = classification_report(y_test, clf_comp_predictions)\n",
    "print(clf_comp_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is **very little difference** between a model with oversampled data and the one with true data. The model with undersampled data does .01 better on recall with class = 2. This is very possibly due to random variation. \n",
    "\n",
    "If two models have identical performance, choose the simpler one. We choose this tuned Random Forest that uses the observed training data with NO resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Settings - Classification of Variable ASOURCE:\n",
    "\n",
    "RandomForestClassifier(n_jobs=-1,max_features=100,n_estimators=300,max_depth=250,min_samples_split=2,min_samples_leaf=1,oob_score = True,random_state=12)\n",
    "                             \n",
    "**Accuracy: ~94.00%**\n",
    "\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          1       0.92      0.97      0.94     24196\n",
    "          2       0.80      0.63      0.71      3456\n",
    "          3       0.72      0.35      0.47      2959\n",
    "          4       0.00      0.00      0.00        23\n",
    "          5       0.96      0.97      0.97     64397\n",
    "\n",
    "    avg / total       0.94      0.94      0.94     95031"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
